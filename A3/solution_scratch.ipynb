{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Input Numbers</th>\n",
       "      <th>Output</th>\n",
       "      <th>algebraic_symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gino has number0 popsicle sticks . i have numb...</td>\n",
       "      <td>what is the sum of our popsicle sticks ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>63 50</td>\n",
       "      <td>113.0</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lino picked up number0 shells at the seashore ...</td>\n",
       "      <td>how many shells did he pick up in all ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>292 324</td>\n",
       "      <td>616.0</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>there were number0 parents in the program and ...</td>\n",
       "      <td>how many people were present in the program ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>105 698</td>\n",
       "      <td>803.0</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>last saturday marie sold number0 magazines and...</td>\n",
       "      <td>what is the total number of reading materials ...</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>425 275</td>\n",
       "      <td>700.0</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>there are number0 birds on the fence . number1...</td>\n",
       "      <td>how many birds are on the fence ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>12 8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>775</td>\n",
       "      <td>when amy got to the fair she had $ number0 . w...</td>\n",
       "      <td>how much money did she spend at the fair ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>15 11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>776</td>\n",
       "      <td>while playing a game kaleb had number0 lives ....</td>\n",
       "      <td>how many lives did kaleb lose ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>98 73</td>\n",
       "      <td>25.0</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>777</td>\n",
       "      <td>maria had number0 pieces of candy . she ate nu...</td>\n",
       "      <td>how many pieces of candy does maria have now ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>67 64</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>778</td>\n",
       "      <td>a store has number0 shirts . after selling som...</td>\n",
       "      <td>how many did they sell ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>49 28</td>\n",
       "      <td>21.0</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>779</td>\n",
       "      <td>billy had number0 cherries . he ate number1 of...</td>\n",
       "      <td>how many cherries does billy have left ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>74 72</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                        Description  \\\n",
       "0             0  gino has number0 popsicle sticks . i have numb...   \n",
       "1             1  lino picked up number0 shells at the seashore ...   \n",
       "2             2  there were number0 parents in the program and ...   \n",
       "3             3  last saturday marie sold number0 magazines and...   \n",
       "4             4  there are number0 birds on the fence . number1...   \n",
       "..          ...                                                ...   \n",
       "775         775  when amy got to the fair she had $ number0 . w...   \n",
       "776         776  while playing a game kaleb had number0 lives ....   \n",
       "777         777  maria had number0 pieces of candy . she ate nu...   \n",
       "778         778  a store has number0 shirts . after selling som...   \n",
       "779         779  billy had number0 cherries . he ate number1 of...   \n",
       "\n",
       "                                              Question           Equation  \\\n",
       "0             what is the sum of our popsicle sticks ?  + number0 number1   \n",
       "1              how many shells did he pick up in all ?  + number0 number1   \n",
       "2        how many people were present in the program ?  + number0 number1   \n",
       "3    what is the total number of reading materials ...  + number0 number1   \n",
       "4                    how many birds are on the fence ?  + number0 number1   \n",
       "..                                                 ...                ...   \n",
       "775         how much money did she spend at the fair ?  - number0 number1   \n",
       "776                    how many lives did kaleb lose ?  - number0 number1   \n",
       "777     how many pieces of candy does maria have now ?  - number0 number1   \n",
       "778                           how many did they sell ?  - number0 number1   \n",
       "779           how many cherries does billy have left ?  - number0 number1   \n",
       "\n",
       "    Input Numbers  Output algebraic_symbols  \n",
       "0           63 50   113.0   [1, 0, 0, 0, 0]  \n",
       "1         292 324   616.0   [1, 0, 0, 0, 0]  \n",
       "2         105 698   803.0   [1, 0, 0, 0, 0]  \n",
       "3         425 275   700.0   [1, 0, 0, 0, 0]  \n",
       "4            12 8    20.0   [1, 0, 0, 0, 0]  \n",
       "..            ...     ...               ...  \n",
       "775         15 11     4.0   [0, 1, 0, 0, 0]  \n",
       "776         98 73    25.0   [0, 1, 0, 0, 0]  \n",
       "777         67 64     3.0   [0, 1, 0, 0, 0]  \n",
       "778         49 28    21.0   [0, 1, 0, 0, 0]  \n",
       "779         74 72     2.0   [0, 1, 0, 0, 0]  \n",
       "\n",
       "[780 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration , BertModel , BertTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchtext import vocab\n",
    "\n",
    "train_dataset_name = 'ArithOps_Train.xlsx'\n",
    "val_dataset_name = 'ArithOps_Validation.xlsx'\n",
    "# df = df.drop('Table 1',axis=1)\n",
    "# df = df.rename(columns=df.iloc[0]).loc[1:]\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "\n",
    "counters = {\"[PAD]\":1,\"<SOS>\":2,\"<EOS>\" : 3 , \"+\" : 4, \"-\" :5 , \"*\" : 6 , \"/\" : 7 }\n",
    "for i in range(10):\n",
    "    counters[\"number\"+str(i)] = i + 8\n",
    "\n",
    "def preprocess_data(dataset_name):\n",
    "    df = pd.read_excel(dataset_name)\n",
    "    algebraic_symbols = []\n",
    "    for i in range(len(df)):\n",
    "        row = str(df.iloc[i]['Equation'])\n",
    "        current_algebraic_symbol = [0 for i in range(5)]\n",
    "        for sym in row.split(' '):\n",
    "            if sym in ['+','-','*','/','%']:\n",
    "                current_algebraic_symbol[counters[sym]-4]+=1\n",
    "        algebraic_symbols.append(str(current_algebraic_symbol))\n",
    "\n",
    "    df['algebraic_symbols'] = algebraic_symbols\n",
    "    return df\n",
    "\n",
    "output_vocabulary = {v: k for k, v in counters.items()}\n",
    "\n",
    "train_df , valid_df = preprocess_data(train_dataset_name), preprocess_data(val_dataset_name)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Input Numbers</th>\n",
       "      <th>Output</th>\n",
       "      <th>algebraic_symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>780</td>\n",
       "      <td>adam had some quarters . he spent number0 of t...</td>\n",
       "      <td>how many quarters did he have to start with ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>9 79</td>\n",
       "      <td>88</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>781</td>\n",
       "      <td>at a bus stop number0 people got off the bus ....</td>\n",
       "      <td>how many people were on the bus before ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>47 43</td>\n",
       "      <td>90</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>782</td>\n",
       "      <td>it takes mike number0 minutes to walk to schoo...</td>\n",
       "      <td>how much time did mike save ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>98 64</td>\n",
       "      <td>34</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>783</td>\n",
       "      <td>a farmer had number0 tomatoes from his garden ...</td>\n",
       "      <td>how many did he pick ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>46 3</td>\n",
       "      <td>43</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>carol had number0 pieces of tissue paper . aft...</td>\n",
       "      <td>how many pieces of tissue paper did she use ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>97 93</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        Description  \\\n",
       "0         780  adam had some quarters . he spent number0 of t...   \n",
       "1         781  at a bus stop number0 people got off the bus ....   \n",
       "2         782  it takes mike number0 minutes to walk to schoo...   \n",
       "3         783  a farmer had number0 tomatoes from his garden ...   \n",
       "4         784  carol had number0 pieces of tissue paper . aft...   \n",
       "\n",
       "                                        Question           Equation  \\\n",
       "0  how many quarters did he have to start with ?  + number0 number1   \n",
       "1       how many people were on the bus before ?  + number0 number1   \n",
       "2                  how much time did mike save ?  - number0 number1   \n",
       "3                         how many did he pick ?  - number0 number1   \n",
       "4  how many pieces of tissue paper did she use ?  - number0 number1   \n",
       "\n",
       "  Input Numbers  Output algebraic_symbols  \n",
       "0          9 79      88   [1, 0, 0, 0, 0]  \n",
       "1         47 43      90   [1, 0, 0, 0, 0]  \n",
       "2         98 64      34   [0, 1, 0, 0, 0]  \n",
       "3          46 3      43   [0, 1, 0, 0, 0]  \n",
       "4         97 93       4   [0, 1, 0, 0, 0]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class T5Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data  : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        text_max_token_length = 512,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data \n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        input_text = data_row[\"Description\"]\n",
    "        input_question = data_row[\"Question\"]\n",
    "\n",
    "        in_text = input_text + \" [SEP] \" + input_question\n",
    "        \n",
    "        input_text_encoding = self.tokenizer(\n",
    "            in_text,\n",
    "            max_length=self.text_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        \n",
    "        output_text = data_row[\"Equation\"]        \n",
    "        output_text = \"<SOS> \" + output_text + \" <EOS>\"\n",
    "        output_tokens = output_text.split()\n",
    "\n",
    "        output_tokens_id_full = torch.zeros((self.output_max_token_length,),dtype=torch.int64)\n",
    "        output_tokens_id = torch.tensor(\n",
    "            [counters[token] for token in output_tokens], dtype=torch.int64)\n",
    "        \n",
    "        output_tokens_id_full[:len(output_tokens)] = output_tokens_id\n",
    "\n",
    "        output_attention_mask = torch.zeros((self.output_max_token_length,))\n",
    "        output_attention_mask[:len(output_tokens_id)] = 1\n",
    "        \n",
    "        output_text_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            max_length=self.output_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        return dict(\n",
    "            input_text = input_text,\n",
    "            output_text = output_text,\n",
    "            input_text_ids = input_text_encoding['input_ids'].flatten(),\n",
    "            input_attention_mask = input_text_encoding['attention_mask'].flatten(),\n",
    "            output_text_ids = output_text_encoding['input_ids'].flatten(),\n",
    "            output_attention_mask = output_text_encoding['attention_mask'].flatten(),\n",
    "            output_text_ids_custom_tokenizer = output_tokens_id_full,\n",
    "            output_attention_mask_custom_tokenizer = output_attention_mask,\n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\",model_max_length=512)\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "t5_model =T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_dataset = T5Dataset(train_df,t5_tokenizer)\n",
    "#valid_dataset = T5Dataset(valid_df,t5_tokenizer)\n",
    "train_dataset = T5Dataset(train_df,bert_tokenizer)\n",
    "valid_dataset = T5Dataset(valid_df,bert_tokenizer)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,32,True)\n",
    "valid_dataloader = DataLoader(valid_dataset,32,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8,  ..., 0, 0, 0],\n",
       "        [2, 7, 9,  ..., 0, 0, 0],\n",
       "        [2, 4, 8,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 5, 9,  ..., 0, 0, 0],\n",
       "        [2, 7, 8,  ..., 0, 0, 0],\n",
       "        [2, 5, 8,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data['output_text_ids_custom_tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfix_evaluation(batch_data,input_values):\n",
    "\n",
    "    arith_symbols = set(['+','-','*','/','%'])\n",
    "    output_values = []\n",
    "    \n",
    "    for i in range(len(batch_data)):\n",
    "        flag = True\n",
    "        current_input = batch_data[i].split(' ')\n",
    "        current_input.reverse()\n",
    "        input_value = input_values[i]\n",
    "\n",
    "        stack = []\n",
    "        for symbol in current_input:\n",
    "            if symbol in arith_symbols:\n",
    "                if len(stack)<2:\n",
    "                    flag = False\n",
    "                    break\n",
    "                in1 = stack.pop(-1)\n",
    "                in2 = stack.pop(-1)\n",
    "\n",
    "                res = 0\n",
    "                if symbol=='+':\n",
    "                    res = in1+in2\n",
    "                elif symbol=='-':\n",
    "                    res = in1 - in2 \n",
    "                elif symbol == '*':\n",
    "                    res = in1 * in2\n",
    "                elif symbol=='/':\n",
    "                    res = in1/in2\n",
    "                else:\n",
    "                    res = in1 % in2\n",
    "                stack.append(res)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if \"number\" in symbol:\n",
    "                    index = int(symbol[6])\n",
    "                    stack.append(input_value[index])\n",
    "\n",
    "        if flag==False or len(stack)!=1:\n",
    "            output_values.append(0)\n",
    "        else:\n",
    "            output_values.append(stack.pop(-1))\n",
    "\n",
    "    ans = torch.tensor(output_values)\n",
    "    return ans\n",
    "\n",
    "ans = postfix_evaluation([\"+ - number0 number1 number2\",\"+ / - number0 number2 number1 number3\"],[[1,4,6],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,dim_model,dropout_p,max_len) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout =  nn.Dropout(dropout_p)\n",
    "\n",
    "        pos_encoding = torch.zeros(max_len,dim_model)\n",
    "        \n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) \n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) \n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        #pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(1), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens_input,\n",
    "        num_tokens_output,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model,\n",
    "            dropout_p= dropout_p,\n",
    "            max_len=5000\n",
    "        )\n",
    "\n",
    "        self.src_embedding = nn.Embedding.from_pretrained(bert_model.embeddings.word_embeddings.weight,freeze=False)\n",
    "        #self.src_embedding = nn.Embedding(num_tokens_input,dim_model)\n",
    "        self.trg_embedding = nn.Embedding(num_tokens_output,dim_model)\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout= dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.dim_model,num_tokens_output)\n",
    "\n",
    "    \n",
    "    def forward(self, src, trg, src_padding_mask=None,target_mask=None, target_padding_mask=None):\n",
    "\n",
    "        src = self.src_embedding(src) * math.sqrt(self.dim_model)\n",
    "        target = self.trg_embedding(trg) * math.sqrt(self.dim_model)\n",
    "        #print(target.shape)\n",
    "        src = self.positional_encoder(src)\n",
    "        target = self.positional_encoder(target)\n",
    "        \n",
    "        transformer_out = self.transformer(\n",
    "            src=  src,tgt = target,tgt_mask=target_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=target_padding_mask\n",
    "        )\n",
    "        out = self.out(transformer_out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def get_tgt_mask(self,size):\n",
    "        \n",
    "        mask = torch.tril(torch.ones(size,size) == 1)\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask==0,float('-inf'))\n",
    "        mask = mask.masked_fill(mask==1,float(0.0))\n",
    "        mask = mask.to(device_fast)\n",
    "        return mask\n",
    "\n",
    "    def get_padding_mask(self,matrix,pad_token):\n",
    "        return (matrix==pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "class TransformerTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens_input,\n",
    "        num_tokens_output,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transformer = TransformerModel(\n",
    "                num_tokens_input=num_tokens_input,\n",
    "                num_tokens_output=num_tokens_output,\n",
    "                dim_model=dim_model,\n",
    "                num_heads=num_heads,\n",
    "                num_encoder_layers=num_encoder_layers,\n",
    "                num_decoder_layers=num_decoder_layers,\n",
    "                dim_feedforward= dim_feedforward,\n",
    "                dropout_p=dropout_p\n",
    "            )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, src, trg, src_padding_mask=None,target_mask=None, target_padding_mask=None):\n",
    "\n",
    "        return self.transformer(src,trg,src_padding_mask,target_mask,target_padding_mask)\n",
    "        \n",
    "\n",
    "    def training_step(self, batch_data,batch_idx):\n",
    "\n",
    "        input_text_ids = batch_data['input_text_ids']\n",
    "        input_attention_mask = batch_data['input_attention_mask']\n",
    "        #output_text_ids = batch_data['output_text_ids']\n",
    "        #output_attention_mask = batch_data['output_attention_mask']\n",
    "\n",
    "        output_text_ids = batch_data['output_text_ids_custom_tokenizer']\n",
    "        output_attention_mask = batch_data['output_attention_mask_custom_tokenizer']\n",
    "        \n",
    "        output_in = output_text_ids[:,:-1]\n",
    "        output_expected = output_text_ids[:,1:]\n",
    "\n",
    "        \n",
    "        target_mask = self.transformer.get_tgt_mask(output_expected.shape[1])\n",
    "\n",
    "        src_padding_mask = self.transformer.get_padding_mask(input_attention_mask,0)\n",
    "        \n",
    "        tgt_padding_mask = self.transformer.get_padding_mask(output_attention_mask[:,:-1],0)\n",
    "\n",
    "\n",
    "        predictions = self(input_text_ids,output_in,src_padding_mask,target_mask,tgt_padding_mask)\n",
    "\n",
    "        loss_value = None\n",
    "        \n",
    "        for i in range(predictions.shape[0]):\n",
    "            if loss_value == None:\n",
    "                loss_value = self.loss_fn(predictions[i],output_expected[i])\n",
    "            else:\n",
    "                loss_value += self.loss_fn(predictions[i],output_expected[i])\n",
    "\n",
    "        train_loss = loss_value*(1.0/predictions.shape[0])\n",
    "\n",
    "\n",
    "        #train_loss = self.loss_fn(predictions,output_expected)\n",
    "        \n",
    "        self.log(\"train_loss\" , train_loss, prog_bar=True,logger=True)\n",
    "  \n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch_data,batch_idx):\n",
    "        \n",
    "        input_text_ids = batch_data['input_text_ids']\n",
    "        input_attention_mask = batch_data['input_attention_mask']\n",
    "        #output_text_ids = batch_data['output_text_ids']\n",
    "        #output_attention_mask = batch_data['output_attention_mask']\n",
    "\n",
    "        output_text_ids = batch_data['output_text_ids_custom_tokenizer']\n",
    "        output_attention_mask = batch_data['output_attention_mask_custom_tokenizer']\n",
    "        \n",
    "        output_in = output_text_ids[:,:-1]\n",
    "        output_expected = output_text_ids[:,1:]\n",
    "\n",
    "        \n",
    "        target_mask = self.transformer.get_tgt_mask(output_expected.shape[1])\n",
    "\n",
    "        src_padding_mask = self.transformer.get_padding_mask(input_attention_mask,0)\n",
    "        \n",
    "        tgt_padding_mask = self.transformer.get_padding_mask(output_attention_mask[:,:-1],0)\n",
    "\n",
    "\n",
    "        predictions = self(input_text_ids,output_in,src_padding_mask,target_mask,tgt_padding_mask)\n",
    "\n",
    "\n",
    "        loss_value = None\n",
    "        \n",
    "        for i in range(predictions.shape[0]):\n",
    "            if loss_value == None:\n",
    "                loss_value = self.loss_fn(predictions[i],output_expected[i])\n",
    "            else:\n",
    "                loss_value += self.loss_fn(predictions[i],output_expected[i])\n",
    "\n",
    "        valid_loss = loss_value*(1.0/predictions.shape[0])\n",
    "        #valid_loss = self.loss_fn(predictions,output_expected)\n",
    "        \n",
    "        self.log(\"valid_loss\" , valid_loss, prog_bar=True,logger=True)\n",
    "  \n",
    "        return valid_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class T5ArithTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, input_attention_mask, decoder_attention_mask, labels):\n",
    "\n",
    "        outs = self.t5_model(input_ids=input_ids,attention_mask = input_attention_mask,labels = labels)        \n",
    "        return outs.loss ,  outs.logits\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"valid_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "No supported gpu backend found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m      6\u001b[0m     dirpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer-scratch-best-checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer_scratch_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformertranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-nlp/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-nlp/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:395\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-nlp/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:143\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_auto_accelerator()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_choose_gpu_accelerator_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_device_config_and_set_final_flags(devices\u001b[38;5;241m=\u001b[39mdevices, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_parallel_devices_and_init_accelerator()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-nlp/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:353\u001b[0m, in \u001b[0;36m_AcceleratorConnector._choose_gpu_accelerator_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CUDAAccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported gpu backend found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: No supported gpu backend found!"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename=\"transformer-scratch-best-checkpoint\",\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode = \"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"transformer_scratch_logs\",name=\"transformertranslator\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks =  checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    log_every_n_steps=5,\n",
    "    accelerator='gpu',\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_tokens_input=30522\n",
    "Num_tokens_output=len(output_vocabulary)\n",
    "Dim_model=768\n",
    "Num_heads=8\n",
    "Num_encoder_layers=6\n",
    "Num_decoder_layers=6\n",
    "Dim_feedforward= 2048\n",
    "Dropout_p=0.1\n",
    "\n",
    "model = TransformerTranslator(\n",
    "    Num_tokens_input,\n",
    "    Num_tokens_output,\n",
    "    Dim_model,\n",
    "    Num_heads,\n",
    "    Num_encoder_layers,\n",
    "    Num_decoder_layers,\n",
    "    Dim_feedforward,\n",
    "    Dropout_p\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,train_dataloader,valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = T5ArithTranslator.load_from_checkpoint(\n",
    "#    '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt'\n",
    "#)\n",
    "'''\n",
    "test_model =  TransformerTranslator.load_from_checkpoint(\n",
    "    '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt'\n",
    ")\n",
    "test_model.freeze()'''\n",
    "\n",
    "Num_tokens_input=30522\n",
    "Num_tokens_output=len(output_vocabulary)\n",
    "Dim_model=768\n",
    "Num_heads=8\n",
    "Num_encoder_layers=6\n",
    "Num_decoder_layers=6\n",
    "Dim_feedforward= 2048\n",
    "Dropout_p=0.1\n",
    "\n",
    "\n",
    "test_model = TransformerTranslator(\n",
    "    Num_tokens_input,\n",
    "    Num_tokens_output,\n",
    "    Dim_model,\n",
    "    Num_heads,\n",
    "    Num_encoder_layers,\n",
    "    Num_decoder_layers,\n",
    "    Dim_feedforward,\n",
    "    Dropout_p\n",
    ")\n",
    "\n",
    "test_model.load_state_dict(torch.load('./checkpoints/transformer-scratch-best-checkpoint.ckpt',map_location=device_fast)[\"state_dict\"])\n",
    "test_model.eval()\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, input_sequence, max_length=128, SOS_token=1, EOS_token=2):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_sequence = input_sequence.to(device_fast)\n",
    "    \n",
    "\n",
    "    y_input = torch.tensor([[1]], dtype=torch.long, device=device_fast)\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.transformer.get_tgt_mask(y_input.size(1)).to(device_fast)\n",
    "        \n",
    "        pred = model(input_sequence, y_input, target_mask=tgt_mask)\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=device_fast)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = bert_tokenizer(\"last stop in their field trip was the aquarium . penny identified number0 species of sharks number1 species of eels and number2 different species of whales . [SEP] how many species was penny able to identify ?\",return_tensors='pt').input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = test_model.to(device_fast)\n",
    "ans = predict(test_model,test_input_ids.to(device_fast))\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = test_model.t5_model.generate(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#print(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
